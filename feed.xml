<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://naspert.github.io/random-thoughts-reboot/feed.xml" rel="self" type="application/atom+xml" /><link href="https://naspert.github.io/random-thoughts-reboot/" rel="alternate" type="text/html" /><updated>2022-01-20T07:26:01-06:00</updated><id>https://naspert.github.io/random-thoughts-reboot/feed.xml</id><title type="html">fastpages</title><subtitle>An easy to use blogging platform with support for Jupyter Notebooks.</subtitle><entry><title type="html">Pre-processing wikipedia dumps with dask (part 2)</title><link href="https://naspert.github.io/random-thoughts-reboot/wikipedia/dask/2020/12/04/daskwiki-part2.html" rel="alternate" type="text/html" title="Pre-processing wikipedia dumps with dask (part 2)" /><published>2020-12-04T00:00:00-06:00</published><updated>2020-12-04T00:00:00-06:00</updated><id>https://naspert.github.io/random-thoughts-reboot/wikipedia/dask/2020/12/04/daskwiki-part2</id><author><name>Nicolas Aspert</name></author><category term="wikipedia" /><category term="dask" /><summary type="html"><![CDATA[Part 1 showed how to convert the SQL into dataframes. Further processing those dataframes into usable data.]]></summary></entry><entry><title type="html">Pre-processing wikipedia dumps with dask (part 1)</title><link href="https://naspert.github.io/random-thoughts-reboot/wikipedia/dask/2020/12/02/daskwiki.html" rel="alternate" type="text/html" title="Pre-processing wikipedia dumps with dask (part 1)" /><published>2020-12-02T00:00:00-06:00</published><updated>2020-12-02T00:00:00-06:00</updated><id>https://naspert.github.io/random-thoughts-reboot/wikipedia/dask/2020/12/02/daskwiki</id><author><name>Nicolas Aspert</name></author><category term="wikipedia" /><category term="dask" /><summary type="html"><![CDATA[Wikipedia SQL dumps are a great data source, of manageable size (when compared to the full dumps). However, processing them efficiently can be challenging. Take advantage of the (hopefully) many CPU cores available to process them. Part 1 shows how to convert the SQL dumps into dataframes and save them as parquet files.]]></summary></entry></feed>