{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing wikipedia dumps with dask (part 1)\n",
    "> Wikipedia SQL dumps are a great data source, of manageable size (when compared to the full dumps). However, processing them efficiently can be challenging. Take advantage of the (hopefully) many CPU cores available to process them. Part 1 shows how to convert the SQL dumps into dataframes and save them as parquet files.\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: Nicolas Aspert\n",
    "- categories: [wikipedia, dask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In addition to being a great source of information, [Wikipedia](https://www.wikipedia.org) is also a model in openness and access to its data. You can access the whole data from the [dumps page](https://dumps.wikimedia.org/) and download most of the data powering Wikipedia. \n",
    "\n",
    "I will focus here on how to process the SQL dumps and transform them to be ingested by, for instance, a [Neo4j graph database](https://neo4j.com). This can be done by other means, for instance using the [sparkwiki](https://github.com/epfl-lts2/sparkwiki) tool (disclaimer: mostly written by me). Sparkwiki is great, however it requires a [spark](https://spark.apache.org/) instance/cluster to work, which is not always easy to set up, even with [Bigtop](https://blog.miz.space/tutorial/2019/04/04/how-to-install-spark-using-apache-bigtop-ubuntu/) or [elasticluster](https://elasticluster.readthedocs.io/en/latest/playbooks.html#hadoop-spark). Nowadays, the research world runs a lot of Python, and [dask](https://dask.org) seems (claims ?) to be the Python-based Spark equivalent, so making an equivalent of sparkwiki in python is a good excuse to see if dask is fit for the job. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "In order to run the experiments below, you need\n",
    "- a good internet connection (the dumps amount for ca. 8 GB of data you need to download)\n",
    "- a computer with a decent amount of RAM and CPUs (I ran them on a 2x20-cores system with 128 GB of RAM, but never used it fully) or a suitable cluster if you have one at hand, and preferrably SSD local storage. I will asssume a Linux-based OS, though it should work on MacOS or even Windows.\n",
    "- a suitable conda/pip environment with the necessary packages installed (TODO)\n",
    "\n",
    "\n",
    "## SQL dumps\n",
    "\n",
    "In this example, I will show how to process the english wikipedia database dump, the adaptation to other languages should be fairly straightforward. You can find the index of all dumps on [this page](https://dumps.wikimedia.org/backup-index.html), the english language SQL dumps are in the [enwiki subfolder](https://dumps.wikimedia.org/enwiki/). In order to achieve speedy downloads, you can use of the [mirrors close to you](https://dumps.wikimedia.org/mirrors.html) (I am in Switzerland, the mirror from UmeÃ¥ university is the fastest one for me, YMMV). The latest backup available (when I wrote this) is from Nov. 20th 2020, I will be using this one in my examples.\n",
    "\n",
    "### Files needed\n",
    "\n",
    "You will need 3 files :\n",
    "- [enwiki-20201120-page.sql.gz](https://dumps.wikimedia.org/enwiki/20201120/enwiki-20201120-page.sql.gz) which contains page data (title, namespace, etc.)\n",
    "- [enwiki-20201120-pagelinks.sql.gz](https://dumps.wikimedia.org/enwiki/20201120/enwiki-20201120-pagelinks.sql.gz) which contains all links between pages within the English wikipedia edition (links across languages are stored in a different table)\n",
    "- [enwiki-20201120-redirect.sql.gz](https://dumps.wikimedia.org/enwiki/20201120/enwiki-20201120-redirect.sql.gz) which contains redirection information betweem pages (e.g when a page is renamed)\n",
    "\n",
    "You can find more detailed information about each table on the [database layout](https://www.mediawiki.org/wiki/Manual:Database_layout) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where data is stored - adjust to your needs\n",
    "data_path = '/data/wikipedia/20201120'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism - preliminary step\n",
    "\n",
    "Sparkwiki was able to process the compressed dumps in parallel after converting them to bz2 archives. After experimenting with dask, it turns out this simple step is not sufficient to achieve parallelism. However, dask supports [reading multiple files in parallel](https://docs.dask.org/en/latest/bag-api.html#dask.bag.read_text), so an extra step is needed to convert the single-file SQL dump into multiple files. Those files can be used to re-create the database powering Wikipedia, and is just a big text file containing `INSERT` statements (quite a lot of them actually). \n",
    "\n",
    "The handy Linux command `split` allows cutting the files into smaller chunks, without breaking SQL statements which need to be parsed later on:\n",
    "```\n",
    "zcat enwiki-20201101-redirect.sql.gz | split -l 100 --filter 'bzip2 > $FILE.bz2' - split-redirect-\n",
    "```\n",
    "\n",
    "This command will cut the redirect SQL dump into 100-lines bzip2-compressed (thanks to the `filter` parameter) chunks with a `split-redirect-` prefix. Check the [man page](https://man7.org/linux/man-pages/man1/split.1.html) of the command for more details.\n",
    "\n",
    "We can now split the two other files. The number of lines in each chunk is hand-adjusted to have a good compromise between number of files and file size :\n",
    "```\n",
    "zcat enwiki-20201101-page.sql.gz | split -l 150 --filter 'bzip2 > $FILE.bz2' - split-page-\n",
    "zcat enwiki-20201101-pagelinks.sql.gz | split -l 400 --filter 'bzip2 > $FILE.bz2' - split-pagelinks-\n",
    "```\n",
    "\n",
    "**This can take a few hours** (esp. the splitting of the pagelinks file), so if you want to skip this part you can use the splits I uploaded on our S3-compatible storage hosted on SwitchEngines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing SQL dumps \n",
    "\n",
    "We are now ready to actually perform the processing. \n",
    "\n",
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import dask.dataframe as ddf\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the dask (local) cluster\n",
    "\n",
    "Before running commands, we need to set up a few things for processing. NB: This is NOT the only way to do it, you can read about setting up the scheduler on a [single machine](https://docs.dask.org/en/latest/setup/single-distributed.html), or on [other types of environments](https://docs.dask.org/en/latest/setup.html) such as clouds, Kubernetes, etc.\n",
    "One important thing to set up when running on a single computer is to have a **local temporary directory**: at some point when processing, dask will write data to disk. It is fairly common to have NFS-mounted directories for instance, and it is crucial for good performance to make sure data does not need to go through a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({'temporary_directory': '/tmp'}) # make sure temp dir is local and has sufficient space. Adjust to your resources/needs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our local cluster. This will work for me, make sure you adapt to your local resources ! It is important to keep $\\text{n_workers} \\times \\text{memory_limit}$ under the physical memory available (or a lower value if the computer is shared with other users and you want to be nice to others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=6, threads_per_worker=4, memory_limit=24e9)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers functions\n",
    "\n",
    "Let us now set up a few routines and regexps (check [sparkwiki](https://github.com/epfl-lts2/sparkwiki) for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dict = {\"page\": \"\\((\\d+),(\\d+),'(.*?)','(.*?)',([01]),([01]),([\\d\\.]+?),'(\\d{14})',(.*?),(\\d+),(\\d+),(.*?),(.*?)\\)\",\n",
    "          \"redirect\":\"\\((\\d+),(\\d+),'(.*?)',(.*?),(.*?)\\)\",\n",
    "          \"pagelinks\":\"\\((\\d+),(\\d+),'(.*?)',(\\d+)\\)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dismiss additional statements (CREATE TABLE, etc.) from the file\n",
    "def filter_line(line, dump_type):\n",
    "    return line.startswith('INSERT INTO `{}` VALUES'.format(dump_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(line):\n",
    "    return line.split(\" VALUES \")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redirect(rec):\n",
    "    return {'from':int(rec[0]), 'target_ns':int(rec[1]), 'title':rec[2], 'inter_wiki':rec[3], 'fragment':rec[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(rec):\n",
    "    # case class WikipediaSimplePage(id:Int, title:String, isRedirect:Boolean, isNew: Boolean)\n",
    "    return {'id': int(rec[0]), 'namespace':int(rec[1]), 'title': rec[2], 'is_redirect':int(rec[4])==1, 'is_new':int(rec[5])==1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagelinks(rec):\n",
    "    # case class WikipediaPagelinks(from:Int, namespace:Int, title:String, fromNamespace:Int)\n",
    "    return {'from': int(rec[0]), 'namespace':int(rec[1]), 'title':rec[2], 'from_namespace':int(rec[3])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing redirects\n",
    "\n",
    "It may seem surprising to start with this particular table. As it is the smallest one, it is often more convenient to start and experiment processing with this one. If you make a mistake, you don't have to wait for a long time before the crash.\n",
    "\n",
    "## Read the local files into a dask bag\n",
    "\n",
    "A [dask bag](https://docs.dask.org/en/latest/bag.html) is the equivalent of a [Spark RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds). It supports simple operations such as `filter`, `map`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects_bag = db.read_text(os.path.join(data_path, 'splits/split-redirect-*.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: read the files from S3 bucket\n",
    "\n",
    "It is not hosted on Amazon but on the Swiss Universities cloud [SwitchEngines](https://www.switch.ch/engines/), hence the custom options needed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_options={'anon':True, 'client_kwargs':{'endpoint_url':'https://os.unil.cloud.switch.ch'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects_bag = db.read_text('s3://lts2-wikipedia/enwiki/20201120/splits/split-redirect-*.bz2', storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tansform each record into a redirect dictionary\n",
    "\n",
    "We can chain conveniently the operators to \n",
    "- filter out non INSERT statements using `filter_line`\n",
    "- split all VALUES from the INSERT using `split_line` and the appropriate regexp\n",
    "- convert to a dictionary using `get_redirect`\n",
    "\n",
    "Sparkwiki does the same operations, check the [WikipediaElementParser.scala](https://github.com/epfl-lts2/sparkwiki/blob/master/src/main/scala/ch/epfl/lts2/wikipedia/WikipediaElementParser.scala) file for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects = redirects_bag.filter(lambda x: filter_line(x, 'redirect'))\\\n",
    "                         .map(split_line)\\\n",
    "                         .map(lambda x:re.findall(re_dict['redirect'], x)).flatten()\\\n",
    "                         .map(get_redirect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the collection of small redirect dictionaries is converted to a dask DataFrame (similar to a pandas DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects_df = redirects.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out all redirects that do not concern namespace 0 (= articles), cf. [Wikipedia namespaces](https://en.wikipedia.org/wiki/Wikipedia:Namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects_df_filt = redirects_df[redirects_df['target_ns']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, nothing is computed yet. Setting the index and saving the resulting DataFrame to a parquet file will trigger all computations. You can monitor what is happening under the hood by opening a connection to the [dask scheduler web interface](https://distributed.dask.org/en/1.9.5/web.html) which should run on port 8787. It takes less than a minute on the system I have at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects_df_filt.set_index('from').to_parquet(os.path.join(data_path, 'processed', 'redirect.parquet'), compression='gzip', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the usual pandas DataFrame methos such as `head`, `tail`etc. However, calling them directly on `redirects_df_filt`will trigger a computation. If you want to avoid this, you can reload it from disk :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_ns</th>\n",
       "      <th>title</th>\n",
       "      <th>inter_wiki</th>\n",
       "      <th>fragment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>Computer_accessibility</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>History_of_Afghanistan</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>Geography_of_Afghanistan</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>Demographics_of_Afghanistan</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>Communications_in_Afghanistan</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target_ns                          title inter_wiki fragment\n",
       "from                                                              \n",
       "10            0         Computer_accessibility         ''       ''\n",
       "13            0         History_of_Afghanistan         ''       ''\n",
       "14            0       Geography_of_Afghanistan         ''       ''\n",
       "15            0    Demographics_of_Afghanistan         ''       ''\n",
       "18            0  Communications_in_Afghanistan         ''       ''"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional\n",
    "redirects_df_reloaded = ddf.read_parquet(os.path.join(data_path, 'processed', 'redirect.parquet'))\n",
    "redirects_df_reloaded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing pages\n",
    "\n",
    "Almost identical to redirects, takes longer though\n",
    "\n",
    "## Read splits from local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_bag = db.read_text(os.path.join(data_path, 'splits/split-page-*.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: read from S3 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_bag = db.read_text('s3://lts2-wikipedia/enwiki/20201120/splits/split-page-*.bz2', storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter records and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pages_bag.filter(lambda x: filter_line(x, 'page'))\\\n",
    "                 .map(split_line)\\\n",
    "                 .map(lambda x:re.findall(re_dict['page'], x))\\\n",
    "                 .flatten().map(get_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = pages.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only namespace 0 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_filt_df = pages_df[pages_df['namespace']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger computation and save result (runs for a few minutes on my system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_filt_df.set_index('id').to_parquet(os.path.join(data_path, 'processed', 'pages.parquet'), compression='gzip', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what we now have in the dataframe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>title</th>\n",
       "      <th>is_redirect</th>\n",
       "      <th>is_new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>AccessibleComputing</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>AfghanistanHistory</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>AfghanistanGeography</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>AfghanistanPeople</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>AfghanistanCommunications</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>AfghanistanTransportations</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>AfghanistanMilitary</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>AfghanistanTransnationalIssues</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>AssistiveTechnology</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>AmoeboidTaxa</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>Autism</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>AlbaniaHistory</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>AlbaniaPeople</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>AsWeMayThink</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>AlbaniaGovernment</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>AlbaniaEconomy</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>Albedo</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>AfroAsiaticLanguages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>ArtificalLanguages</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    namespace                           title  is_redirect  is_new\n",
       "id                                                                \n",
       "10          0             AccessibleComputing         True   False\n",
       "12          0                       Anarchism        False   False\n",
       "13          0              AfghanistanHistory         True   False\n",
       "14          0            AfghanistanGeography         True   False\n",
       "15          0               AfghanistanPeople         True   False\n",
       "18          0       AfghanistanCommunications         True   False\n",
       "19          0      AfghanistanTransportations         True   False\n",
       "20          0             AfghanistanMilitary         True   False\n",
       "21          0  AfghanistanTransnationalIssues         True   False\n",
       "23          0             AssistiveTechnology         True   False\n",
       "24          0                    AmoeboidTaxa         True   False\n",
       "25          0                          Autism        False   False\n",
       "27          0                  AlbaniaHistory         True   False\n",
       "29          0                   AlbaniaPeople         True   False\n",
       "30          0                    AsWeMayThink         True   False\n",
       "35          0               AlbaniaGovernment         True   False\n",
       "36          0                  AlbaniaEconomy         True   False\n",
       "39          0                          Albedo        False   False\n",
       "40          0            AfroAsiaticLanguages         True   False\n",
       "42          0              ArtificalLanguages         True   False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional reload from disk\n",
    "pages_df_reloaded = ddf.read_parquet(os.path.join(data_path, 'processed', 'pages.parquet'))\n",
    "pages_df_reloaded.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing pagelinks\n",
    "\n",
    "NB: this is the biggest part to process. It seems some entries can trigger utf-8 decoding errors, hence the `errors='backslashreplace'` addition whem reading.\n",
    "No index will be created to speed up computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelinks_bag = db.read_text(os.path.join(data_path, 'splits/split-pagelinks-*.bz2'), errors='backslashreplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or S3-hosted splits :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageslinks_bag = db.read_text('s3://lts2-wikipedia/enwiki/20201120/splits/split-pagelinks-*.bz2', storage_options=storage_options, errors='backslashreplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get pagelinks dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelinks = pagelinks_bag.filter(lambda x: filter_line(x, 'pagelinks'))\\\n",
    "                         .map(split_line)\\\n",
    "                         .map(lambda x:re.findall(re_dict['pagelinks'], x))\\\n",
    "                         .flatten().map(get_pagelinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelinks_df = pagelinks.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep links between articles (namespace == 0), discard all the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelinks_filt_df = pagelinks_df[(pagelinks_df['namespace'] == 0) & (pagelinks_df['from_namespace'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelinks_filt_df.to_parquet(os.path.join(data_path, 'processed', 'pagelinks.parquet'), compression='gzip', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>namespace</th>\n",
       "      <th>title</th>\n",
       "      <th>from_namespace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4748</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9773</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15154</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25213</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>613303</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1028188</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1497620</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2875276</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2988645</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4355567</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5583438</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7712754</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9969569</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11646457</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20481393</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21855996</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23752827</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33983238</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>35557493</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>35678765</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        from  namespace title  from_namespace\n",
       "0       4748          0     !               0\n",
       "1       9773          0     !               0\n",
       "2      15154          0     !               0\n",
       "3      25213          0     !               0\n",
       "4     613303          0     !               0\n",
       "5    1028188          0     !               0\n",
       "6    1497620          0     !               0\n",
       "7    2875276          0     !               0\n",
       "8    2988645          0     !               0\n",
       "9    4355567          0     !               0\n",
       "10   5583438          0     !               0\n",
       "11   7712754          0     !               0\n",
       "12   9969569          0     !               0\n",
       "13  11646457          0     !               0\n",
       "14  20481393          0     !               0\n",
       "15  21855996          0     !               0\n",
       "16  23752827          0     !               0\n",
       "17  33983238          0     !               0\n",
       "18  35557493          0     !               0\n",
       "19  35678765          0     !               0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optional reload from disk\n",
    "pagelinks_reloaded = ddf.read_parquet(os.path.join(data_path, 'processed', 'pagelinks.parquet'))\n",
    "pagelinks_reloaded.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
