{
  
    
        "post0": {
            "title": "Pre-processing wikipedia dumps with dask (part 2)",
            "content": "After processing the SQL dumps in part 1, we need to further process the data to make it importable in Neo4j. We need to . find the id of the pages when redirecting | find the target id of the page in the pagelinks table, taking into account page redirections | . There are other ways of doing those tasks, and handling redirects can be done directly in Neo4j: you can imagine creating the redirection pages and have links of type REDIRECTS_TO in addition to the &quot;normal&quot; links between pages LINKS_TO. However, this would add a huge number of relationships in the database (I tried it for you), and makes the queries slower and more complex, without bringing (at least for the studies performed in our lab) additional value. Hence the choice of handling redirections prior import. . Make sure to check the sparkwiki import guide to get a global view. The source code of DumpProcessor is also of interest, as we will replicate its functionalities here. . import os import pandas as pd import dask import dask.dataframe as ddf from dask.distributed import LocalCluster, Client . dask.config.set({&#39;temporary_directory&#39;: &#39;/tmp&#39;}) # make sure temp dir is local ! . &lt;dask.config.set at 0x7f984190c150&gt; . cluster = LocalCluster(n_workers=6, threads_per_worker=4, memory_limit=24e9) # adapt to your local resources ! client = Client(cluster) . data_path = &#39;/data/wikipedia/20201120&#39; . Read parquet files generated in part 1 from disk . redirect_df = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;redirect.parquet&#39;)) pages_df = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pages.parquet&#39;)) pagelinks_df = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks.parquet&#39;)) . Or read my copy from our s3 bucket on switchengines . storage_options={&#39;anon&#39;:True, &#39;client_kwargs&#39;:{&#39;endpoint_url&#39;:&#39;https://os.unil.cloud.switch.ch&#39;}} . redirect_df = ddf.read_parquet(&#39;s3://lts2-wikipedia/enwiki/20201120/redirect.parquet&#39;, storage_options=storage_options) pages_df = ddf.read_parquet(&#39;s3://lts2-wikipedia/enwiki/20201120/pages.parquet&#39;, storage_options=storage_options) pagelinks_df = ddf.read_parquet(&#39;s3://lts2-wikipedia/enwiki/20201120/pagelinks.parquet&#39;, storage_options=storage_options) . Just a quick glance at the dataframe to make sure the data is here . redirect_df.head(10) . target_ns title inter_wiki fragment . from . 10 0 | Computer_accessibility | &#39;&#39; | &#39;&#39; | . 13 0 | History_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 14 0 | Geography_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 15 0 | Demographics_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 18 0 | Communications_in_Afghanistan | &#39;&#39; | &#39;&#39; | . 19 0 | Transport_in_Afghanistan | &#39;&#39; | &#39;&#39; | . 20 0 | Afghan_Armed_Forces | &#39;&#39; | &#39;&#39; | . 21 0 | Foreign_relations_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 23 0 | Assistive_technology | &#39;&#39; | &#39;&#39; | . 24 0 | Amoeba | &#39;&#39; | &#39;&#39; | . pages_df.head(10) . namespace title is_redirect is_new . id . 10 0 | AccessibleComputing | True | False | . 12 0 | Anarchism | False | False | . 13 0 | AfghanistanHistory | True | False | . 14 0 | AfghanistanGeography | True | False | . 15 0 | AfghanistanPeople | True | False | . 18 0 | AfghanistanCommunications | True | False | . 19 0 | AfghanistanTransportations | True | False | . 20 0 | AfghanistanMilitary | True | False | . 21 0 | AfghanistanTransnationalIssues | True | False | . 23 0 | AssistiveTechnology | True | False | . pagelinks_df.head(10) . from namespace title from_namespace . 0 4748 | 0 | ! | 0 | . 1 9773 | 0 | ! | 0 | . 2 15154 | 0 | ! | 0 | . 3 25213 | 0 | ! | 0 | . 4 613303 | 0 | ! | 0 | . 5 1028188 | 0 | ! | 0 | . 6 1497620 | 0 | ! | 0 | . 7 2875276 | 0 | ! | 0 | . 8 2988645 | 0 | ! | 0 | . 9 4355567 | 0 | ! | 0 | . Merge redirect with pages . We need to find the id of the page we are redirecting to. E.g. page with id=10 &#39;AccessibleComputing&#39; redirects to &#39;Computer_accessibility&#39; . pages_df[pages_df[&#39;title&#39;] == &#39;Computer_accessibility&#39;].compute() . namespace title is_redirect is_new . id . 411964 0 | Computer_accessibility | False | False | . Here is how sparkwiki does the join between redirect and page DataFrames. We can skip the join on the namespace as we only kept namespace 0 in our pre-processed DataFrames (sparkwiki was designed to keep categories so additional care needs to be taken to get the proper id). . redirectDf.withColumn(&quot;id&quot;, redirectDf.col(&quot;from&quot;)) .join(pageDf.drop(pageDf.col(&quot;title&quot;)), &quot;id&quot;) .select(&quot;from&quot;, &quot;targetNamespace&quot;, &quot;title&quot;) .withColumn(&quot;namespace&quot;, redirectDf.col(&quot;targetNamespace&quot;)) .join(pageDf, Seq(&quot;title&quot;, &quot;namespace&quot;)).select(&quot;from&quot;, &quot;id&quot;, &quot;title&quot;) .as[MergedRedirect] . Here is the equivalent in python. The reset_index calls are needed because we need the indices in the resulting dataframe. Since we only kept namespace 0 pages, we can join using the title field only. . redirect_merged = redirect_df.reset_index().merge(pages_df.reset_index(), left_on=&#39;title&#39;, right_on=&#39;title&#39;) .drop(columns=[&#39;inter_wiki&#39;, &#39;fragment&#39;, &#39;namespace&#39;, &#39;is_new&#39;, &#39;target_ns&#39;]) . We have still have leftover redirects that are not fully resolved . redirect_merged[redirect_merged[&#39;is_redirect&#39;]].head(20, npartitions=10) . from title id is_redirect . 101499 12242480 | New_Mexico_State_Road_333 | 2788393 | True | . 232021 65663584 | Senator_Shapiro_(disambiguation) | 65663584 | True | . 214813 56753023 | Ran_Neu-Ner | 51045453 | True | . 229234 64811911 | Monastery_of_Loukous | 65734014 | True | . 220021 59393217 | Micro_TDH | 59393217 | True | . 231248 65733619 | AnTuTu/11_Pro_Max_VS_S20_VS_Xperia_1_ii_VS_Fin... | 65733623 | True | . 229301 63009285 | Fa_Hai | 17826148 | True | . 229302 63371040 | Fa_Hai | 17826148 | True | . 232399 65521541 | Centipetalism | 65730750 | True | . 232400 65524209 | Centipetalism | 65730750 | True | . 229236 61459055 | Baseco,_Manila | 65734038 | True | . 229237 61459107 | Baseco,_Manila | 65734038 | True | . 229238 64901313 | Baseco,_Manila | 65734038 | True | . 235946 65731901 | Indigenous_land_conflicts_on_the_Mexico–United... | 65731974 | True | . 170127 37065249 | Kunthavai_Nachiaar_College | 65734319 | True | . 227222 65386007 | COVID-19_pandemic_in_Door_County,_Wisconsin | 65282057 | True | . Let us just discard them for now... . redirect_merged_filt = redirect_merged[~redirect_merged[&#39;is_redirect&#39;]] . redirect_merged_filt.to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;redirect_merged.parquet&#39;), compression=&#39;gzip&#39;) . Merge pagelinks . This is a bit more complicated, two steps are needed. . Step 1 . Find the id of the target page based on the title field, quite similar to the redirects we merged before. Here is the sparkwiki version : . val pagelinks_id = pagelinks.join(pages, Seq(&quot;title&quot;, &quot;namespace&quot;)) .select(&quot;from&quot;, &quot;id&quot;, &quot;title&quot;, &quot;fromNamespace&quot;, &quot;namespace&quot;) . pagelinks_id = pagelinks_df.drop(columns=[&#39;namespace&#39;, &#39;from_namespace&#39;]).merge(pages_df.drop(columns=[&#39;namespace&#39;]).reset_index() .drop(columns=[&#39;is_new&#39;]), left_on=&#39;title&#39;, right_on=&#39;title&#39;) . Since we left out anything not being in namespace 0, a merge on the title field is sufficient. . pagelinks_id.head() # Expensive ! . from title id is_redirect . 0 364060 | !Kung_languages | 63813576 | True | . 1 453953 | !Kung_languages | 63813576 | True | . 2 453979 | !Kung_languages | 63813576 | True | . 3 453991 | !Kung_languages | 63813576 | True | . 4 1210028 | !Kung_languages | 63813576 | True | . . pagelinks_id.to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks_id.parquet&#39;), compression=&#39;gzip&#39;) . pagelinks_id[pagelinks_id[&#39;from&#39;]==774551].head(20) . from title id is_redirect . 314 774551 | (10397)_1997_SX33 | 33450953 | True | . 319 774551 | (121014)_1999_AJ22 | 33715493 | True | . 320 774551 | (12821)_1996_RG1 | 33476015 | True | . 321 774551 | (136838)_1997_WG22 | 33716060 | True | . 325 774551 | (15901)_1997_RY8 | 33496407 | True | . 443 774551 | (20261)_1998_FM12 | 33508561 | True | . 591 774551 | (246880)_1995_SR54 | 33737413 | True | . 593 774551 | (264290)_1998_SD27 | 33737645 | True | . 594 774551 | (26987)_1997_WP1 | 33517118 | True | . 602 774551 | (31177)_1997_XH11 | 33538369 | True | . 708 774551 | (43886)_1995_GR7 | 33583541 | True | . 709 774551 | (47129)_1999_CR118 | 33612119 | True | . 716 774551 | (55870)_1997_TD26 | 33647299 | True | . 721 774551 | (69478)_1996_XO15 | 33652547 | True | . 725 774551 | (73979)_1998_DF8 | 33659593 | True | . 727 774551 | (8227)_1996_VD4 | 33446736 | True | . 729 774551 | (85504)_1997_TC26 | 33678855 | True | . 730 774551 | (85659)_1998_QU29 | 33684922 | True | . 732 774551 | (90919)_1997_PA5 | 33687137 | True | . 733 774551 | (96300)_1996_SC8 | 33687729 | True | . Step 2 . Now the more tricky part is to resolve redirections. . val linksDf = pgLinksIdDf.withColumn(&quot;inter&quot;, pgLinksIdDf.col(&quot;id&quot;)) .join(redirectDf.withColumn(&quot;inter&quot;, redirectDf.col(&quot;from&quot;)).withColumnRenamed(&quot;from&quot;, &quot;from_r&quot;).withColumnRenamed(&quot;id&quot;, &quot;to_r&quot;), Seq(&quot;inter&quot;), &quot;left&quot;) .withColumn(&quot;dest&quot;, when(col(&quot;to_r&quot;).isNotNull, col(&quot;to_r&quot;)).otherwise(col(&quot;id&quot;))) .select(&quot;from&quot;, &quot;dest&quot;) .filter($&quot;from&quot; !== $&quot;dest&quot;) // remove self-links .distinct // redirect removal will cause duplicates -&gt; remove them . Reload the computed dataframes from disk to avoid computations... . pagelinks_id = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks_id.parquet&#39;)) redirect_merged = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;redirect_merged.parquet&#39;)) . ...or fetch them from S3 . redirect_merged = ddf.read_parquet(&#39;s3://lts2-wikipedia/enwiki/20201120/redirect_merged.parquet&#39;, storage_options=storage_options) pagelinks_id = ddf.read_parquet(&#39;s3://lts2-wikipedia/enwiki/20201120/pagelinks_id.parquet&#39;, storage_options=storage_options) . Join pagelinks and redirect . Let us see how our data looks like after the left join . pagelinks_redir_merge = pagelinks_id.merge(redirect_merged.reset_index(), left_on=[&#39;id&#39;], right_on=[&#39;from&#39;], how=&#39;left&#39;) . pagelinks_redir_merge.head(20) . from_x title_x id_x is_redirect_x index from_y title_y id_y is_redirect_y . 0 18301841 | .xsi | 26107610 | True | 75539.0 | 26107610.0 | Autodesk_Softimage | 1931049.0 | False | . 1 23790514 | 1001st_Helicopter_Squadron | 54167065 | True | 136648.0 | 54167065.0 | 1st_Helicopter_Squadron | 8659503.0 | False | . 2 415160 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 3 425895 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 4 3042106 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 5 9540507 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 6 9648456 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 7 9648459 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 8 9648466 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 9 9648476 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 10 10980141 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 11 18552607 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 12 18585691 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 13 28409618 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 14 29526040 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 15 31814001 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 16 32190319 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 17 32215624 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 18 32471803 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . 19 34514250 | 10th_Infantry_Division_(Greece) | 34514441 | True | 165820.0 | 34514441.0 | 10th_Mechanized_Infantry_Brigade_(Greece) | 34514432.0 | False | . We need to keep id id_x column when no redirection exists, and the id_y when the page is redirected. This is done by creating a new column (I did not find how to do it using a single statement as the original Scala code does) . pagelinks_redir_merge[&#39;id_merge&#39;] = pagelinks_redir_merge[&#39;id_x&#39;].where(pagelinks_redir_merge[&#39;id_y&#39;].isnull(), pagelinks_redir_merge[&#39;id_y&#39;]) . pagelinks_redir_merge_final = pagelinks_redir_merge.drop(columns=[&#39;id_x&#39;, &#39;index&#39;, &#39;title_x&#39;, &#39;id_y&#39;, &#39;title_y&#39;, &#39;from_y&#39;, &#39;is_redirect_x&#39;, &#39;is_redirect_y&#39;]) .rename(columns={&#39;id_merge&#39;:&#39;dest&#39;, &#39;from_x&#39;:&#39;from&#39;}) . Self-links removal . The resolution of redirections created self-links . self_links = pagelinks_redir_merge_final[pagelinks_redir_merge_final[&#39;from&#39;]==pagelinks_redir_merge_final[&#39;dest&#39;]] . self_links.head() . from dest . 21 34514432 | 34514432 | . 333 11632630 | 11632630 | . 3473 63932158 | 63932158 | . 3575 45449270 | 45449270 | . 10134 30136816 | 30136816 | . Remove the self-links, as they are not useful. . pagelinks_redir_noself = pagelinks_redir_merge_final[pagelinks_redir_merge_final[&#39;from&#39;] != pagelinks_redir_merge_final[&#39;dest&#39;]] . Last step is to remove links from pages that are redirects, as does sparkwiki: . // some redirect pages have regular links -&gt; remove them linksDf.withColumn(&quot;id&quot;, linksDf.col(&quot;from&quot;)) .join(pages, &quot;id&quot;) .filter($&quot;isRedirect&quot; === false) .select(&quot;from&quot;, &quot;dest&quot;) . pagelinks_redir_pages = pagelinks_redir_noself.merge(pages_df, left_on=&#39;from&#39;, right_index=True) .drop(columns=[&#39;namespace&#39;, &#39;title&#39;, &#39;is_new&#39;]) . Ignore links on redirect pages . Remove links from pages having is_redirect is true . pagelinks_redir_clean = pagelinks_redir_pages[~pagelinks_redir_pages[&#39;is_redirect&#39;]].drop(columns=[&#39;is_redirect&#39;]) . And finally save the result, always useful to have a checkpoint to restart from if things go bad . pagelinks_redir_clean.to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks_redir_clean.parquet&#39;), compression=&#39;gzip&#39;) . Cleanup - duplicate links removal . If you read carefully, you noticed that the distinct call of the first Scala/Spark query. Dask has a drop_duplicates that sound to be what we want. . First, let us re-read the processing we did so far from disk to avoid recomputation . pagelinks_redir_clean = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks_redir_clean.parquet&#39;)) . As always, our S3 copy is available . pagelinks_redir_clean = ddf.read_parquet(&#39;s3://lts2-wikipedia/enwiki/20201120/pagelinks_redir_clean.parquet&#39;, storage_options=storage_options) . pagelinks_redir_clean.count().compute() . from 534689528 dest 534689528 dtype: int64 . That is almost 535 millions of links. . pagelinks_redir_nodupes = pagelinks_redir_clean.drop_duplicates([&#39;from&#39;, &#39;dest&#39;]) . Trying to run the code below might trigger out-of-memory errors and finally the process will fail if your workers do not have more than 55GB to 60GB memory available ! You have been warned :) ! A repartition call has been added to avoid ending up with a single file. . pagelinks_redir_nodupes.repartition(npartitions=50) .to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks_redir_nodupes.parquet&#39;), compression=&#39;gzip&#39;) . Play it safe, reload from disk ... . pagelinks_redir_nodupes = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks_redir_nodupes.parquet&#39;), engine=&#39;pyarrow&#39;) . ... or from S3 . pagelinks_redir_nodupes = ddf.read_parquet(&#39;s3://lts2-wikipedia/enwiki/20201120/pagelinks_redir_nodupes.parquet&#39;, storage_options=storage_options) . pagelinks_redir_nodupes.count().compute() . from 522219227 dest 522219227 dtype: int64 . There was about 15 millions duplicate links removed. Quite worth the computing time . Conclusions . Dask is a great tool to exploit parallelism of your local computer, without the hassle of setting up Spark. All you have to do is create a conda environment, install dask and you are mostly done. However, while writing this, I compared with a dockerized spark instance, running the sparkwiki processing rewritten for pyspark (each worker was allocated 20GB of RAM and 10 cores). It seems that, for now, spark is more efficient. The redirection processing took only a few minutes and at least twice longer using the dask equivalents and no memory trouble was seen, whereas the duplicates removal required a much bigger amount of memory per worker. . Let us compare our results with the pyspark-processed version: . pagelinks_pyspark = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks_ps_clean.parquet&#39;), engine=&#39;pyarrow&#39;) . pagelinks_pyspark.count().compute() . from 522219227 dest 522219227 dtype: int64 . Good news, same amount of data in the pyspark version ! Looks like we managed to process in a similar way. . links_ps = pagelinks_pyspark[pagelinks_pyspark[&#39;dest&#39;]==1677].compute() . links_dask = pagelinks_redir_nodupes[pagelinks_redir_nodupes[&#39;dest&#39;]==1677].compute() . links_ps.sort_values(&#39;from&#39;).head() . from dest . 764 736 | 1677 | . 743 1676 | 1677 | . 2063 1688 | 1677 | . 1167 1689 | 1677 | . 363 1862 | 1677 | . delta = (links_dask.sort_values(&#39;from&#39;).reset_index()[&#39;from&#39;]-links_ps.sort_values(&#39;from&#39;).reset_index()[&#39;from&#39;]) . delta.min() . 0 . delta.max() . 0 .",
            "url": "https://naspert.github.io/random-thoughts-reboot/wikipedia/dask/2020/12/04/daskwiki-part2.html",
            "relUrl": "/wikipedia/dask/2020/12/04/daskwiki-part2.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Pre-processing wikipedia dumps with dask (part 1)",
            "content": "Introduction . In addition to being a great source of information, Wikipedia is also a model in openness and access to its data. You can access the whole data from the dumps page and download most of the data powering Wikipedia. . I will focus here on how to process the SQL dumps and transform them to be ingested by, for instance, a Neo4j graph database. This can be done by other means, for instance using the sparkwiki tool (disclaimer: mostly written by me). Sparkwiki is great, however it requires a spark instance/cluster to work, which is not always easy to set up, even with Bigtop or elasticluster. Nowadays, the research world runs a lot of Python, and dask seems (claims ?) to be the Python-based Spark equivalent, so making an equivalent of sparkwiki in python is a good excuse to see if dask is fit for the job. . Requirements . In order to run the experiments below, you need . a good internet connection (the dumps amount for ca. 8 GB of data you need to download) | a computer with a decent amount of RAM and CPUs (I ran them on a 2x20-cores system with 128 GB of RAM, but never used it fully) or a suitable cluster if you have one at hand, and preferrably SSD local storage. I will asssume a Linux-based OS, though it should work on MacOS or even Windows. | a suitable conda/pip environment with the necessary packages installed (TODO) | . SQL dumps . In this example, I will show how to process the english wikipedia database dump, the adaptation to other languages should be fairly straightforward. You can find the index of all dumps on this page, the english language SQL dumps are in the enwiki subfolder. In order to achieve speedy downloads, you can use of the mirrors close to you (I am in Switzerland, the mirror from Umeå university is the fastest one for me, YMMV). The latest backup available (when I wrote this) is from Nov. 20th 2020, I will be using this one in my examples. . Files needed . You will need 3 files : . enwiki-20201120-page.sql.gz which contains page data (title, namespace, etc.) | enwiki-20201120-pagelinks.sql.gz which contains all links between pages within the English wikipedia edition (links across languages are stored in a different table) | enwiki-20201120-redirect.sql.gz which contains redirection information betweem pages (e.g when a page is renamed) | . You can find more detailed information about each table on the database layout page. . data_path = &#39;/data/wikipedia/20201120&#39; . Parallelism - preliminary step . Sparkwiki was able to process the compressed dumps in parallel after converting them to bz2 archives. After experimenting with dask, it turns out this simple step is not sufficient to achieve parallelism. However, dask supports reading multiple files in parallel, so an extra step is needed to convert the single-file SQL dump into multiple files. Those files can be used to re-create the database powering Wikipedia, and is just a big text file containing INSERT statements (quite a lot of them actually). . The handy Linux command split allows cutting the files into smaller chunks, without breaking SQL statements which need to be parsed later on: . zcat enwiki-20201101-redirect.sql.gz | split -l 100 --filter &#39;bzip2 &gt; $FILE.bz2&#39; - split-redirect- . This command will cut the redirect SQL dump into 100-lines bzip2-compressed (thanks to the filter parameter) chunks with a split-redirect- prefix. Check the man page of the command for more details. . We can now split the two other files. The number of lines in each chunk is hand-adjusted to have a good compromise between number of files and file size : . zcat enwiki-20201101-page.sql.gz | split -l 150 --filter &#39;bzip2 &gt; $FILE.bz2&#39; - split-page- zcat enwiki-20201101-pagelinks.sql.gz | split -l 400 --filter &#39;bzip2 &gt; $FILE.bz2&#39; - split-pagelinks- . This can take a few hours (esp. the splitting of the pagelinks file), so if you want to skip this part you can use the splits I uploaded on our S3-compatible storage hosted on SwitchEngines. . Processing SQL dumps . We are now ready to actually perform the processing. . Imports . import pandas as pd import dask import dask.bag as db import dask.dataframe as ddf import re import os . Starting the dask (local) cluster . Before running commands, we need to set up a few things for processing. NB: This is NOT the only way to do it, you can read about setting up the scheduler on a single machine, or on other types of environments such as clouds, Kubernetes, etc. One important thing to set up when running on a single computer is to have a local temporary directory: at some point when processing, dask will write data to disk. It is fairly common to have NFS-mounted directories for instance, and it is crucial for good performance to make sure data does not need to go through a network. . dask.config.set({&#39;temporary_directory&#39;: &#39;/tmp&#39;}) # make sure temp dir is local and has sufficient space. Adjust to your resources/needs ! . from dask.distributed import LocalCluster, Client . We can now start our local cluster. This will work for me, make sure you adapt to your local resources ! It is important to keep $ text{n_workers} times text{memory_limit}$ under the physical memory available (or a lower value if the computer is shared with other users and you want to be nice to others). . cluster = LocalCluster(n_workers=6, threads_per_worker=4, memory_limit=24e9) client = Client(cluster) . Helpers functions . Let us now set up a few routines and regexps (check sparkwiki for details) . re_dict = {&quot;page&quot;: &quot; (( d+),( d+),&#39;(.*?)&#39;,&#39;(.*?)&#39;,([01]),([01]),([ d .]+?),&#39;( d{14})&#39;,(.*?),( d+),( d+),(.*?),(.*?) )&quot;, &quot;redirect&quot;:&quot; (( d+),( d+),&#39;(.*?)&#39;,(.*?),(.*?) )&quot;, &quot;pagelinks&quot;:&quot; (( d+),( d+),&#39;(.*?)&#39;,( d+) )&quot;} . def filter_line(line, dump_type): return line.startswith(&#39;INSERT INTO `{}` VALUES&#39;.format(dump_type)) . def split_line(line): return line.split(&quot; VALUES &quot;)[1].strip() . def get_redirect(rec): return {&#39;from&#39;:int(rec[0]), &#39;target_ns&#39;:int(rec[1]), &#39;title&#39;:rec[2], &#39;inter_wiki&#39;:rec[3], &#39;fragment&#39;:rec[4]} . def get_page(rec): # case class WikipediaSimplePage(id:Int, title:String, isRedirect:Boolean, isNew: Boolean) return {&#39;id&#39;: int(rec[0]), &#39;namespace&#39;:int(rec[1]), &#39;title&#39;: rec[2], &#39;is_redirect&#39;:int(rec[4])==1, &#39;is_new&#39;:int(rec[5])==1} . def get_pagelinks(rec): # case class WikipediaPagelinks(from:Int, namespace:Int, title:String, fromNamespace:Int) return {&#39;from&#39;: int(rec[0]), &#39;namespace&#39;:int(rec[1]), &#39;title&#39;:rec[2], &#39;from_namespace&#39;:int(rec[3])} . Processing redirects . It may seem surprising to start with this particular table. As it is the smallest one, it is often more convenient to start and experiment processing with this one. If you make a mistake, you don&#39;t have to wait for a long time before the crash. . Read the local files into a dask bag . A dask bag is the equivalent of a Spark RDD. It supports simple operations such as filter, map, etc. . redirects_bag = db.read_text(os.path.join(data_path, &#39;splits/split-redirect-*.bz2&#39;) . Alternative: read the files from S3 bucket . It is not hosted on Amazon but on the Swiss Universities cloud SwitchEngines, hence the custom options needed below . storage_options={&#39;anon&#39;:True, &#39;client_kwargs&#39;:{&#39;endpoint_url&#39;:&#39;https://os.unil.cloud.switch.ch&#39;}} . redirects_bag = db.read_text(&#39;s3://lts2-wikipedia/enwiki/20201120/splits/split-redirect-*.bz2&#39;, storage_options=storage_options) . Tansform each record into a redirect dictionary . We can chain conveniently the operators to . filter out non INSERT statements using filter_line | split all VALUES from the INSERT using split_line and the appropriate regexp | convert to a dictionary using get_redirect | . Sparkwiki does the same operations, check the WikipediaElementParser.scala file for more details. . redirects = redirects_bag.filter(lambda x: filter_line(x, &#39;redirect&#39;)) .map(split_line) .map(lambda x:re.findall(re_dict[&#39;redirect&#39;], x)).flatten() .map(get_redirect) . Finally, the collection of small redirect dictionaries is converted to a dask DataFrame (similar to a pandas DataFrame) . redirects_df = redirects.to_dataframe() . Filter out all redirects that do not concern namespace 0 (= articles), cf. Wikipedia namespaces . redirects_df_filt = redirects_df[redirects_df[&#39;target_ns&#39;]==0] . At this point, nothing is computed yet. Setting the index and saving the resulting DataFrame to a parquet file will trigger all computations. You can monitor what is happening under the hood by opening a connection to the dask scheduler web interface which should run on port 8787. It takes less than a minute on the system I have at hand. . redirects_df_filt.set_index(&#39;from&#39;).to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;redirect.parquet&#39;), compression=&#39;gzip&#39;, engine=&#39;fastparquet&#39;) . You can call the usual pandas DataFrame methos such as head, tailetc. However, calling them directly on redirects_df_filtwill trigger a computation. If you want to avoid this, you can reload it from disk : . redirects_df_reloaded = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;redirect.parquet&#39;)) redirects_df_reloaded.head() . target_ns title inter_wiki fragment . from . 10 0 | Computer_accessibility | &#39;&#39; | &#39;&#39; | . 13 0 | History_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 14 0 | Geography_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 15 0 | Demographics_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 18 0 | Communications_in_Afghanistan | &#39;&#39; | &#39;&#39; | . Processing pages . Almost identical to redirects, takes longer though . Read splits from local filesystem . pages_bag = db.read_text(os.path.join(data_path, &#39;splits/split-page-*.bz2&#39;) . Alternative: read from S3 storage . pages_bag = db.read_text(&#39;s3://lts2-wikipedia/enwiki/20201120/splits/split-page-*.bz2&#39;, storage_options=storage_options) . Filter records and create dataframe . pages = pages_bag.filter(lambda x: filter_line(x, &#39;page&#39;)) .map(split_line) .map(lambda x:re.findall(re_dict[&#39;page&#39;], x)) .flatten().map(get_page) . pages_df = pages.to_dataframe() . Keep only namespace 0 pages . pages_filt_df = pages_df[pages_df[&#39;namespace&#39;]==0] . Trigger computation and save result (runs for a few minutes on my system) . pages_filt_df.set_index(&#39;id&#39;).to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pages.parquet&#39;), compression=&#39;gzip&#39;, engine=&#39;fastparquet&#39;) . Check what we now have in the dataframe : . pages_df_reloaded = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pages.parquet&#39;)) pages_df_reloaded.head(20) . namespace title is_redirect is_new . id . 10 0 | AccessibleComputing | True | False | . 12 0 | Anarchism | False | False | . 13 0 | AfghanistanHistory | True | False | . 14 0 | AfghanistanGeography | True | False | . 15 0 | AfghanistanPeople | True | False | . 18 0 | AfghanistanCommunications | True | False | . 19 0 | AfghanistanTransportations | True | False | . 20 0 | AfghanistanMilitary | True | False | . 21 0 | AfghanistanTransnationalIssues | True | False | . 23 0 | AssistiveTechnology | True | False | . 24 0 | AmoeboidTaxa | True | False | . 25 0 | Autism | False | False | . 27 0 | AlbaniaHistory | True | False | . 29 0 | AlbaniaPeople | True | False | . 30 0 | AsWeMayThink | True | False | . 35 0 | AlbaniaGovernment | True | False | . 36 0 | AlbaniaEconomy | True | False | . 39 0 | Albedo | False | False | . 40 0 | AfroAsiaticLanguages | True | False | . 42 0 | ArtificalLanguages | True | False | . Processing pagelinks . NB: this is the biggest part to process. It seems some entries can trigger utf-8 decoding errors, hence the errors=&#39;backslashreplace&#39; addition whem reading. No index will be created to speed up computation . pagelinks_bag = db.read_text(os.path.join(data_path, &#39;splits/split-pagelinks-*.bz2&#39;), errors=&#39;backslashreplace&#39;) . Or S3-hosted splits : . pageslinks_bag = db.read_text(&#39;s3://lts2-wikipedia/enwiki/20201120/splits/split-pagelinks-*.bz2&#39;, storage_options=storage_options, errors=&#39;backslashreplace&#39;) . get pagelinks dictionaries . pagelinks = pagelinks_bag.filter(lambda x: filter_line(x, &#39;pagelinks&#39;)) .map(split_line) .map(lambda x:re.findall(re_dict[&#39;pagelinks&#39;], x)) .flatten().map(get_pagelinks) . pagelinks_df = pagelinks.to_dataframe() . Only keep links between articles (namespace == 0), discard all the others . pagelinks_filt_df = pagelinks_df[(pagelinks_df[&#39;namespace&#39;] == 0) &amp; (pagelinks_df[&#39;from_namespace&#39;] == 0)] . pagelinks_filt_df.to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks.parquet&#39;), compression=&#39;gzip&#39;, engine=&#39;fastparquet&#39;) . pagelinks_reloaded = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks.parquet&#39;)) pagelinks_reloaded.head(20) . from namespace title from_namespace . 0 4748 | 0 | ! | 0 | . 1 9773 | 0 | ! | 0 | . 2 15154 | 0 | ! | 0 | . 3 25213 | 0 | ! | 0 | . 4 613303 | 0 | ! | 0 | . 5 1028188 | 0 | ! | 0 | . 6 1497620 | 0 | ! | 0 | . 7 2875276 | 0 | ! | 0 | . 8 2988645 | 0 | ! | 0 | . 9 4355567 | 0 | ! | 0 | . 10 5583438 | 0 | ! | 0 | . 11 7712754 | 0 | ! | 0 | . 12 9969569 | 0 | ! | 0 | . 13 11646457 | 0 | ! | 0 | . 14 20481393 | 0 | ! | 0 | . 15 21855996 | 0 | ! | 0 | . 16 23752827 | 0 | ! | 0 | . 17 33983238 | 0 | ! | 0 | . 18 35557493 | 0 | ! | 0 | . 19 35678765 | 0 | ! | 0 | .",
            "url": "https://naspert.github.io/random-thoughts-reboot/wikipedia/dask/2020/12/02/daskwiki.html",
            "relUrl": "/wikipedia/dask/2020/12/02/daskwiki.html",
            "date": " • Dec 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Nicolas Aspert, I work as researcher/IT manager at the Signal Processing Laboratory 2 in EPFL. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://naspert.github.io/random-thoughts-reboot/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://naspert.github.io/random-thoughts-reboot/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}